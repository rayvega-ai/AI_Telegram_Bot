import os
import asyncio
import json
import logging
from typing import Optional
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.types import BotCommand
from aiogram.utils.chat_action import ChatActionSender
from dotenv import load_dotenv
import google.generativeai as genai

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò ---
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
MY_ID = 6055791149

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 2. –õ–û–ì–ò–ö–ê –ê–í–¢–û–ü–û–î–ë–û–†–ê –ú–û–î–ï–õ–ò (–¢–í–û–ô –ö–û–î) ---
genai.configure(api_key=GEMINI_KEY)

async def list_models_safe():
    try:
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º loop.run_in_executor –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤, 
        # —Ç–∞–∫ –∫–∞–∫ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –æ–Ω –æ–±—ã—á–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π
        return genai.list_models()
    except Exception as e:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: %s", e)
        return []

def choose_available_model(models_iterable, preferred_keywords=("gemini", "flash", "2.0", "3.0")) -> Optional[str]:
    available = []
    for m in models_iterable:
        name = getattr(m, "name", None) or getattr(m, "model", None) or None
        methods = getattr(m, "supported_generation_methods", None) or []
        available.append({"name": name, "methods": methods})

    logger.info("–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: %d", len(available))

    for entry in available:
        name = entry["name"]
        methods = entry["methods"]
        if not name: continue
        if any(k in name.lower() for k in preferred_keywords) and any(
            m in methods for m in ("generateContent", "chat", "sendMessage", "send_message")
        ):
            logger.info("–í—ã–±—Ä–∞–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: %s", name)
            return name

    for entry in available:
        name = entry["name"]
        if name and "generateContent" in entry["methods"]:
            return name
    return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    models_list = list(genai.list_models())
    SELECTED_MODEL = choose_available_model(models_list)
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
    SELECTED_MODEL = "models/gemini-1.5-flash-latest"

if not SELECTED_MODEL:
    SELECTED_MODEL = "models/gemini-1.5-flash-latest"
    logger.warning("–ò—Å–ø–æ–ª—å–∑—É—é fallback: %s", SELECTED_MODEL)

def get_model(is_admin: bool = False):
    base_prompt = "–¢—ã ‚Äî –ò–ò Gemini. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ, –∫—Ä–∞—Ç–∫–æ –∏ –±–µ–∑ –≤–æ–¥—ã."
    system_instruction = f"–ú–∞–∫—Å–∏–º ‚Äî —Ç–≤–æ–π —Å–æ–∑–¥–∞—Ç–µ–ª—å. {base_prompt}" if is_admin else f"{base_prompt} –¢—ã —Å–æ–∑–¥–∞–Ω –ú–∞–∫—Å–∏–º–æ–º."
    try:
        return genai.GenerativeModel(model_name=SELECTED_MODEL, system_instruction=system_instruction)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ GenerativeModel: {e}")
        return genai.GenerativeModel(model_name=SELECTED_MODEL)

# --- 3. –§–ê–ô–õ–´ –ò –ë–û–¢ ---
HISTORY_FILE = "memory.json"
bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher()

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_history(data):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# --- 4. –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò ---
@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer("–ú–∞–∫—Å–∏–º, —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –ú–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")

@dp.message(F.text)
async def handle_message(message: types.Message):
    uid = str(message.from_user.id)
    async with ChatActionSender.typing(bot=bot, chat_id=message.chat.id):
        try:
            histories = load_history()
            user_history = histories.get(uid, [])
            
            model = get_model(is_admin=(int(uid) == MY_ID))
            chat = model.start_chat(history=user_history)
            
            response = await asyncio.to_thread(chat.send_message, message.text)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é (–∫—Ä–∞—Ç–∫–æ)
            new_history = []
            for content in chat.history[-10:]:
                new_history.append({"role": content.role, "parts": [p.text for p in content.parts]})
            
            histories[uid] = new_history
            save_history(histories)

            await message.answer(response.text, parse_mode="Markdown")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
            await message.answer("‚ö†Ô∏è –û—à–∏–±–∫–∞. –ü—Ä–æ–≤–µ—Ä—å API –∫–ª—é—á –∏–ª–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.")

# --- 5. –ó–ê–ü–£–°–ö ---
async def main():
    await bot.delete_webhook(drop_pending_updates=True)
    print(f"üöÄ –ó–ê–ü–£–°–ö. –í–´–ë–†–ê–ù–ù–ê–Ø –ú–û–î–ï–õ–¨: {SELECTED_MODEL}")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
